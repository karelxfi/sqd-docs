---
title: Development flow
description: A general approach to squid development
---

This page is a definitive end-to-end guide into practical squid development. It uses templates to simplify the process. Check out [Squid from scratch](/en/sdk/squid-sdk/evm/getting-started/squid-from-scratch) for a more educational barebones approach.

<Info>
  Feel free to also use the template-specific `sqd` scripts defined in
  [`commands.json`](/sdk/squid-cli/commands-json) to simplify your workflow. See
  [sqd CLI cheatsheet](/en/sdk/squid-sdk/evm/getting-started/cli-cheatsheet) for a
  short intro.
</Info>

## Prepare the environment

- Node v20.x or newer
- Git
- [Squid CLI](/sdk/squid-cli/installation)
- Docker (if your squid will store its data to PostgreSQL)

See also the [Environment set up](/en/sdk/squid-sdk/evm/getting-started/development-environment-set-up) page.

## Understand your technical requirements

Consider your business requirements and find out

1. How the data should be delivered. Options:
   - [PostgreSQL](/sdk/squid-sdk/data-persistence/typeorm) with an optional [GraphQL API](/sdk/squid-sdk/data-serving/serving-graphql) - can be real-time
   - [file-based dataset](/sdk/squid-sdk/data-persistence/file) - local or on S3
   - [Google BigQuery](/sdk/squid-sdk/data-persistence/bigquery)
2. What data should be delivered
3. What are the technologies powering the blockchain(s) in question. Supported options:

   - Ethereum Virtual Machine (EVM) chains like [Ethereum](https://ethereum.org) - [supported networks](/data/networks)

   Note that you can use SQD via [RPC ingestion](/sdk/squid-sdk/common-topics/unfinalized-blocks) even if your network is not listed.

4. What exact data should be retrieved from blockchain(s)
5. Whether you need to mix in any [off-chain data](/sdk/squid-sdk/common-topics/external-api)

#### Example requirements

<details>
<summary>DEX analytics on Polygon</summary>

Suppose you want to train a prototype ML model on all trades done on Uniswap Polygon since the v3 upgrade.

1. A delay of a few hours typically won't matter for training, so you may want to deliver the data as files for easier handling.
2. The output could be a simple list of swaps, listing pair, direction and token amounts for each.
3. Polygon is an EVM chain.
4. All the required data is contained within `Swap` events emitted by the pair pool contracts. Uniswap deploys these [dynamically](/sdk/squid-sdk/evm/factory-contracts), so you will also have to capture `PoolCreated` events from the factory contract to know which `Swap` events are coming from Uniswap and map them to pairs.
5. No off-chain data will be necessary for this task.

</details>

<details>
<summary>NFT ownership on Ethereum</summary>

Suppose you want to make a website that shows the image and ownership history for ERC721 NFTs from a certain Ethereum contract.

1. For this application it makes sense to deliver a GraphQL API.
2. Output data might have `Token`, `Owner` and `Transfer` database tables / [entities](/sdk/squid-sdk/reference/schema-file/entities), with e.g. `Token` supplying all the fields necessary to show ownership history and the image.
3. Ethereum is an EVM chain.
4. Data on token mints and ownership history can be derived from `Transfer(address,address,uint256)` EVM event logs emitted by the contract. To render images, you will also need token metadata URLs that are only available by [querying the contract state](/sdk/squid-sdk/evm/typegen/state-queries) with the `tokenURI(uint256)` function.
5. You'll need to retrieve the off-chain token metadata (usually from IPFS).

</details>

## Start from a template \{#templates\}

Although it is possible to [compose a squid from individual packages](/en/sdk/squid-sdk/evm/getting-started/squid-from-scratch), in practice it is usually easier to start from a template.

<AccordionGroup>
<Accordion title="Templates for the PostgreSQL+GraphQL data destination">

- A minimal template intended for developing EVM squids. Indexes ETH burns.

```bash
sqd init my-squid-name -t evm
```

- A starter squid for indexing ERC20 transfers.

```bash
sqd init my-squid-name -t https://github.com/subsquid-labs/squid-erc20-template
```

- Classic [example Subgraph](https://github.com/graphprotocol/example-subgraph) after a [migration](/sdk/squid-sdk/how-to-migrate/migrate-subgraph) to SQD.

```bash
sqd init my-squid-name -t gravatar
```

- A template showing how to [combine data from multiple chains](/sdk/squid-sdk/common-topics/multichain). Indexes USDC transfers on Ethereum and Binance.

```bash
sqd init my-squid-name -t multichain
```

</Accordion>

<Accordion title="Templates for storing data in files">

- USDC transfers -> local CSV

```bash
sqd init my-squid-name -t https://github.com/subsquid-labs/file-store-csv-example
```

- USDC transfers -> local Parquet

```bash
sqd init my-squid-name -t https://github.com/subsquid-labs/file-store-parquet-example
```

- USDC transfers -> CSV on S3

```bash
sqd init my-squid-name -t https://github.com/subsquid-labs/file-store-s3-example
```

</Accordion>

<Accordion title="Templates for the Google BigQuery data destination">

- USDC transfers -> BigQuery dataset

```bash
sqd init my-squid-name -t https://github.com/subsquid-labs/squid-bigquery-example
```

</Accordion>
</AccordionGroup>

After retrieving the template of choice install its dependencies:

```bash
cd my-squid-name
npm i
```

Test the template locally. The procedure varies depending on the data sink:

<Tabs>
<Tab title="PostgreSQL+GraphQL">

<Steps>
<Step title="Launch a PostgreSQL container">
```bash
docker compose up -d
```
</Step>

<Step title="Build the squid">```bash npm run build ```</Step>

<Step title="Apply the DB migrations">
  ```bash npx squid-typeorm-migration apply ```
</Step>

<Step title="Start the squid processor">
```bash
node -r dotenv/config lib/main.js
```

You should see output that contains lines like these ones:

```bash
04:11:24 INFO  sqd:processor processing blocks from 6000000
04:11:24 INFO  sqd:processor using archive data source
04:11:24 INFO  sqd:processor prometheus metrics are served at port 45829
04:11:27 INFO  sqd:processor 6051219 / 18079056, rate: 16781 blocks/sec, mapping: 770 blocks/sec, 544 items/sec, eta: 12m
```

</Step>

<Step title="Start the GraphQL server">
Run the following command in a separate terminal:

```bash
npx squid-graphql-server
```

Then visit the [GraphiQL console](http://localhost:4350/graphql) to verify that the GraphQL API is up.

</Step>
</Steps>

When done, shut down and erase your database with `docker compose down`.

</Tab>

<Tab title="filesystem dataset">

<Steps>
<Step title="Set up credentials (S3 template only)">
For the S3 template only, set the credentials and prepare a bucket for your data as described in the [template README](https://github.com/subsquid-labs/file-store-s3-example/blob/main/README.md).
</Step>

<Step title="Build the squid">```bash npm run build ```</Step>

<Step title="Start the squid processor">
```bash
node -r dotenv/config lib/main.js
```

The output should contain lines like these ones:

```bash
04:11:24 INFO  sqd:processor processing blocks from 6000000
04:11:24 INFO  sqd:processor using archive data source
04:11:24 INFO  sqd:processor prometheus metrics are served at port 45829
04:11:27 INFO  sqd:processor 6051219 / 18079056, rate: 16781 blocks/sec, mapping: 770 blocks/sec, 544 items/sec, eta: 12m
```

You should see a `./data` folder populated with indexer data appear in a bit. A local folder looks like this:

```bash
$ tree ./data/
./data/
├── 0000000000-0007242369
│   └── transfers.tsv
├── 0007242370-0007638609
│   └── transfers.tsv
...
└── status.txt
```

</Step>
</Steps>

</Tab>

<Tab title="BigQuery">

Create a dataset with your BigQuery account, then follow the [template README](https://github.com/subsquid-labs/squid-bigquery-example/blob/master/README.md).

</Tab>
</Tabs>

## The bottom-up development cycle \{#bottom-up-development\}

The advantage of this approach is that the code remains buildable at all times, making it easier to catch issues early.

### I. Regenerate the task-specific utilities \{#typegen\}

Retrieve JSON ABIs for all contracts of interest (e.g. from Etherscan), taking care to get ABIs for implementation contracts and not [proxies](/sdk/squid-sdk/evm/proxy-contracts) where appropriate. Assuming that you saved the ABI files to `./abi`, you can then regenerate the utilities with

```bash
npx squid-evm-typegen ./src/abi ./abi/*.json --multicall
```

Or if you would like the tool to retrieve the ABI from Etherscan in your stead, you can run e.g.

```bash
npx squid-evm-typegen \
  src/abi \
  0xdAC17F958D2ee523a2206206994597C13D831ec7#usdt
```

The utility classes will become available at `src/abi`.

See also [EVM typegen code generation](/sdk/squid-sdk/evm/typegen/generation).

### II. Configure the data requests \{#processor-config\}

Data requests are [customarily](/en/sdk/squid-sdk/evm/getting-started/layout) defined at `src/processor.ts`.

Edit the definition of `const processor` to:

1. Use a data source appropriate for your chain and task.

   - It is possible to [use RPC](/sdk/squid-sdk/evm/reference/batch-processor/general#set-rpc-endpoint) as the only data source, but [adding](/sdk/squid-sdk/evm/reference/batch-processor/general#set-gateway) a [SQD Network](/data/networks) data source will make your squid sync much faster.
   - RPC is a hard requirement if you're building a real-time API.
   - If you're using RPC as one of your data sources, make sure to [set the number of finality confirmations](/sdk/squid-sdk/evm/reference/batch-processor/general#set-finality-confirmation) so that [hot blocks ingestion](/sdk/squid-sdk/common-topics/unfinalized-blocks) works properly.
   - On low block time, high data rate networks (e.g. Arbitrum) use WSS endpoints if latency is critical.

2. Request all [event logs](/sdk/squid-sdk/evm/reference/batch-processor/logs), [transactions](/sdk/squid-sdk/evm/reference/batch-processor/transactions), [execution traces](/sdk/squid-sdk/evm/reference/batch-processor/traces) and [state diffs](/sdk/squid-sdk/evm/reference/batch-processor/state-diffs) that your task requires, with any necessary related data (e.g. parent transactions for event logs).

3. [Select all data fields](/sdk/squid-sdk/evm/reference/batch-processor/field-selection) necessary for your task (e.g. `gasUsed` for transactions).

See [reference documentation](/sdk/squid-sdk/evm/reference/batch-processor/overview) for more info and [processor configuration showcase](/sdk/squid-sdk/examples) for a representative set of examples.

### III. Decode and normalize the data \{#batch-handler-decoding\}

Next, change the batch handler to decode and normalize your data.

In templates, the batch handler is defined at the [`processor.run()`](/sdk/squid-sdk/reference/processors/architecture#processorrun) call in `src/main.ts` as an inline function. Its sole argument `ctx` contains:

- at `ctx.blocks`: all the requested data for a batch of blocks
- at `ctx.store`: the means to save the processed data
- at `ctx.log`: a [`Logger`](/sdk/squid-sdk/reference/logger)
- at `ctx.isHead`: a boolean indicating whether the batch is at the current chain head
- at `ctx._chain`: the means to access RPC for [state calls](#external-data)

This structure ([reference](/sdk/squid-sdk/reference/processors/architecture#batch-context)) is common for all processors.

Each item in `ctx.blocks` contains the data for the requested logs, transactions, traces and state diffs for a particular block, plus some info on the block itself. See [EVM batch context reference](/sdk/squid-sdk/evm/reference/batch-processor/context-interfaces).

Use the `.decode` methods from the [contract ABI utilities](#typegen) to decode events and transactions, e.g.

```typescript
import * as erc20abi from "./abi/erc20";

processor.run(db, async (ctx) => {
  for (let block of ctx.blocks) {
    for (let log of block.logs) {
      if (log.topics[0] === erc20abi.events.Transfer.topic) {
        let { from, to, value } = erc20.events.Transfer.decode(log);
      }
    }
  }
});
```

See also the [EVM data decoding](/sdk/squid-sdk/evm/typegen/decoding).

### (Optional) IV. Mix in external data and chain state calls output \{#external-data\}

If you need external (i.e. non-blockchain) data in your transformation, take a look at the [External APIs and IPFS](/sdk/squid-sdk/common-topics/external-api) page.

If any of the on-chain data you need is unavalable from the processor or incovenient to retrieve with it, you have an option to get it via [direct chain queries](/sdk/squid-sdk/evm/typegen/state-queries).

### V. Prepare the store \{#store\}

At `src/main.ts`, change the [`Database`](/sdk/squid-sdk/data-persistence/overview) object definition to accept your output data. The methods for saving data will be exposed by `ctx.store` within the [batch handler](/sdk/squid-sdk/reference/processors/architecture).

<Tabs>
<Tab title="PostgreSQL+GraphQL">

<Steps>
<Step title="Define the database schema">
Define the schema of the database (and the [core schema of the OpenReader GraphQL API](/sdk/squid-sdk/reference/openreader-server/api) if it is used) at [`schema.graphql`](/sdk/squid-sdk/reference/schema-file).
</Step>

<Step title="Regenerate the TypeORM model classes">
```bash
npx squid-typeorm-codegen
```

The classes will become available at `src/model`.

</Step>

<Step title="Compile the models code">```bash npm run build ```</Step>

<Step title="Ensure access to a blank database">
The easiest way to do so is to start PostgreSQL in a Docker container with:

```bash
docker compose up -d
```

If the container is running, stop it and erase the database with:

```bash
docker compose down
```

before issuing a `docker compose up -d`.

<Note>
The alternative is to connect to an external database. See [this section](/sdk/squid-sdk/reference/store/typeorm#database-connection-parameters) to learn how to specify the connection parameters.
</Note>
</Step>

<Step title="Regenerate a migration">
```bash
rm -r db/migrations
```
```bash
npx squid-typeorm-migration generate
```
</Step>
</Steps>

You can now use the async functions [`ctx.store.upsert()`](/sdk/squid-sdk/reference/store/typeorm#upsert) and [`ctx.store.insert()`](/sdk/squid-sdk/reference/store/typeorm#insert), as well as various [TypeORM lookup methods](/sdk/squid-sdk/reference/store/typeorm#typeorm-methods) to access the database.

See the `typeorm-store` [guide](/sdk/squid-sdk/data-persistence/typeorm) and [reference](/sdk/squid-sdk/reference/store/typeorm) for more info.

</Tab>

<Tab title="filesystem dataset">

Filesystem dataset writing, as performed by the `@subsquid/file-store` package and its extensions, stores the data into one or more flat tables. The exact table definition format depends on the output file format.

<Steps>
<Step title="Decide on the file format">
Choose from:
- [Parquet](/sdk/squid-sdk/reference/store/file/parquet)
- [CSV](/sdk/squid-sdk/reference/store/file/csv)
- [JSON/JSONL](/sdk/squid-sdk/reference/store/file/json)

If your template does not have any of the necessary packages, install them.

</Step>

<Step title="Define tables">
Define any tables you need at the `tables` field of the `Database` constructor argument:

```typescript
import { Database } from '@subsquid/file-store'

const dbOptions = {
  tables: {
    FirstTable: new Table(/* ... */),
    SecondTable: new Table(/* ... */),
    // ...
  },
  // ...
}

processor.run(new Database(dbOptions), async ctx => { // ...
```

</Step>

<Step title="Define the destination filesystem">
Configure the `dest` field of the `Database` constructor argument. Options:
- local folder - use `LocalDest` from `@subsquid/file-store`
- S3-compatible file storage service - install `@subsquid/file-store-s3` and use [`S3Dest`](/sdk/squid-sdk/reference/store/file/s3-dest)
</Step>
</Steps>

Once you're done you'll be able to enqueue data rows for saving using the `write()` and `writeMany()` methods of the context store-provided table objects:

```typescript
ctx.store.FirstTable.writeMany(/* ... */);
ctx.store.SecondTable.write(/* ... */);
```

The store will write the files automatically as soon as the buffer reaches the size set by the `chunkSizeMb` field of the `Database` constructor argument, or at the end of the batch if a call to [`setForceFlush()`](/sdk/squid-sdk/data-persistence/file#setforceflush) was made anywhere in the batch handler.

See the `file-store` [guide](/sdk/squid-sdk/data-persistence/file) and the [reference pages of its extensions](/sdk/squid-sdk/reference/store/file).

</Tab>

<Tab title="BigQuery">

Follow the [guide](/sdk/squid-sdk/data-persistence/bigquery).

</Tab>
</Tabs>

### VI. Persist the transformed data to your data sink \{#batch-handler-persistence\}

Once your data is [decoded](#batch-handler-decoding), optionally [enriched with external data](#external-data) and transformed the way you need it to be, it is time to save it.

<Tabs>
<Tab title="PostgreSQL+GraphQL">

For each batch, create all the instances of all TypeORM model classes at once, then save them with the minimal number of calls to `upsert()` or `insert()`, e.g.:

```typescript
import { EntityA, EntityB } from "./model";

processor.run(new TypeormDatabase(), async (ctx) => {
  const aEntities: Map<string, EntityA> = new Map(); // id -> entity instance
  const bEntities: EntityB = [];

  for (let block of ctx.blocks) {
    // fill the containets aEntities and bEntities
  }

  await ctx.store.upsert([...aEntities.values()]);
  await ctx.store.insert(bEntities);
});
```

It will often make sense to keep the entity instances in maps rather than arrays to make it easier to reuse them when defining instances of other entities with [relations](/sdk/squid-sdk/reference/schema-file/entity-relations) to the previous ones. The process is described in more detail in the [step 2 of the BAYC tutorial](/sdk/squid-sdk/tutorials/bayc/step-two-deriving-owners-and-tokens).

If you perform any [database lookups](/sdk/squid-sdk/reference/store/typeorm#typeorm-methods), try to do so in batches and make sure that the entity fields that you're searching over are [indexed](/sdk/squid-sdk/reference/schema-file/indexes-and-constraints).

See also the [patterns](/sdk/squid-sdk/common-topics/batch-processing#patterns) and [anti-pattens](/sdk/squid-sdk/common-topics/batch-processing#anti-patterns) sections of the Batch processing guide.

</Tab>

<Tab title="filesystem dataset">

You can enqueue the transformed data for writing whenever convenient without any sizeable impact on performance.

At low output data rates (e.g. if your entire dataset is in tens of Mbytes or under) take care to call [`ctx.store.setForceFlush()`](/sdk/squid-sdk/data-persistence/file#setforceflush) when appropriate to make sure your data actually gets written.

</Tab>

<Tab title="BigQuery">

You can enqueue the transformed data for writing whenever convenient without any sizeable impact on performance. The actual data writing will happen automatically at the end of each batch.

</Tab>
</Tabs>

## The top-down development cycle

The [bottom-up development cycle](#bottom-up-development) described above is convenient for inital squid development and for trying out new things, but it has the disadvantage of not having the means of saving the data ready at hand when initially writing the data decoding/transformation code. That makes it necessary to come back to that code later, which is somewhat inconvenient e.g. when adding new squid features incrementally.

The alternative is to do the same steps in a different order:

1. [Update the store](#store)
2. If necessary, [regenerate the utility classes](#typegen)
3. [Update the processor configuration](#processor-config)
4. [Decode and normalize the added data](#batch-handler-decoding)
5. [Retrieve any external data](#external-data) if necessary
6. [Add the persistence code for the transformed data](#batch-handler-persistence)

## GraphQL options

[Store your data to PostgreSQL](/sdk/squid-sdk/data-persistence/typeorm), then consult [Serving GraphQL](/sdk/squid-sdk/data-serving/serving-graphql) for options.

## Scaling up

If you're developing a large squid, make sure to use [batch processing](/sdk/squid-sdk/common-topics/batch-processing) throughout your code.

A common mistake is to make handlers for individual event logs or transactions; for updates that require data retrieval that results in lots of small database lookups and ultimately in poor syncing performance. Collect all the relevant data and process it at once. A simple architecture of that type is discussed in the [BAYC tutorial](/sdk/squid-sdk/tutorials/bayc).

You should also check the [Cloud best practices page](/cloud/resources/best-practices) even if you're not planning to deploy to [SQD Cloud](/cloud) - it contains valuable performance-related tips.

Many issues commonly arising when developing larger squids are addressed by the third party [`@belopash/typeorm-store` package](/other/external-tools#belopashtypeorm-store). Consider using it.

For complete examples of complex squids take a look at the [Giant Squid Explorer](https://github.com/subsquid-labs/giant-squid-explorer) and [Thena Squid](https://github.com/subsquid-labs/thena-squid) repos.

## Next steps

- Learn about [batch processing](/sdk/squid-sdk/common-topics/batch-processing).
- Learn how squid deal with [unfinalized blocks](/sdk/squid-sdk/common-topics/unfinalized-blocks).
- [Use external APIs and IPFS](/sdk/squid-sdk/common-topics/external-api) in your squid.
- See how squid should be set up for the [multichain setting](/sdk/squid-sdk/common-topics/multichain).
- Deploy your squid [on own infrastructure](/sdk/squid-sdk/deployment/self-hosting) or to [SQD Cloud](/cloud).
